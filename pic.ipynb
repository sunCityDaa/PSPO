{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model and tokenizer...\n",
      "Loading base model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 71.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA configuration...\n",
      "Loading LoRA model and applying weights...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM, TextIteratorStreamer, GenerationConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "import torch\n",
    "import copy\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "base_model_dir = \"/data/model/Qwen2.5-Math-7B\"\n",
    "lora_model_dir = \"/data/ER-GRPO/Qwen2.5-Math-7B-data/GRPO/checkpoint-100\"\n",
    "device_map = \"cpu\"\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "print(\"Loading base model and tokenizer...\")\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_dir,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    "   \n",
    "\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_dir)\n",
    "model_base_original = copy.deepcopy(model_base)\n",
    "\n",
    "print(\"Loading LoRA configuration...\")\n",
    "peft_config = PeftConfig.from_pretrained(lora_model_dir)\n",
    "\n",
    "print(\"Loading LoRA model and applying weights...\")\n",
    "model_lora = PeftModel.from_pretrained(\n",
    "    model_base,\n",
    "    lora_model_dir,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    "    is_trainable=True,\n",
    "    inference_mode=False # 明确禁用推理模式，保证LoRA激活\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Merging LoRA weights into base model...\")\n",
    "model_merged = model_lora.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokenizer.json', 'merges.txt', 'adapter_config.json', 'latest', 'chat_template.jinja', 'added_tokens.json', 'vocab.json', 'tokenizer_config.json', 'rng_state_1.pth', 'scheduler.pt', 'zero_to_fp32.py', 'README.md', 'rng_state_0.pth', 'global_step100', 'adapter_model.safetensors', 'trainer_state.json', 'training_args.bin', 'special_tokens_map.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(lora_model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA modules: {'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/data/model/Qwen2.5-Math-7B', revision=None, inference_mode=False, r=32, target_modules={'down_proj', 'up_proj', 'v_proj', 'o_proj', 'q_proj', 'gate_proj', 'k_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n"
     ]
    }
   ],
   "source": [
    "print(\"LoRA modules:\", model_lora.peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dict keys: dict_keys(['base_model.model.model.layers.0.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.24.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.24.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.24.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.24.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.24.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.25.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.25.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.25.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.25.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.25.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.26.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.26.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.26.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.26.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.26.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.27.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.27.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.27.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.27.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.27.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.weight'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "print(\"Loaded state dict keys:\", state_dict.keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compare_model_weights(model1, model2):\n",
    "    for name1, param1 in model1.named_parameters():\n",
    "        if name1 in model2.state_dict():\n",
    "            param2 = model2.state_dict()[name1]\n",
    "            if not torch.allclose(param1, param2, rtol=1e-03, atol=1e-03):\n",
    "                print(f\"Layer '{name1}': Weights are DIFFERENT.\")\n",
    "                return True\n",
    "        else:\n",
    "            print(f\"Layer '{name1}' not found in the second model.\")\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_model_weights(model_base_original, model_merged)  # Should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = list(model_lora.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are not equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n",
      "Parameters are equal.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 获取参数\n",
    "params1 = list(model_base_original.parameters())\n",
    "params2 = list(model_merged.parameters())\n",
    "\n",
    "# 比较参数\n",
    "for p1, p2 in zip(params1, params2):\n",
    "    if torch.allclose(p1.data, p2.data):\n",
    "        print(\"Parameters are equal.\")\n",
    "    else:\n",
    "        print(\"Parameters are not equal.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter model.layers.0.self_attn.q_proj.weight differs\n",
      "Models have different parameters\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "params1 = {name: param.clone().detach() for name, param in model_base_original.named_parameters()}\n",
    "params2 = {name: param.clone().detach() for name, param in model_merged.named_parameters()}\n",
    "are_same = True\n",
    "for name in params1.keys():\n",
    "    if name not in params2:\n",
    "        print(f\"Parameter {name} not found in model2\")\n",
    "        are_same = False\n",
    "        break\n",
    "    if not torch.allclose(params1[name], params2[name], atol=1e-6):\n",
    "        print(f\"Parameter {name} differs\")\n",
    "        are_same = False\n",
    "        break\n",
    "\n",
    "if are_same:\n",
    "    print(\"All parameters are identical\")\n",
    "else:\n",
    "    print(\"Models have different parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
